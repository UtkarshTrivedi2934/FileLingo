{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xohmbYy6bvxM",
        "outputId": "884f9a6f-130d-4dde-e0ce-9a783c998991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDkxjG3X8qfR"
      },
      "source": [
        "#**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhOr0zJ2cNXu",
        "outputId": "591f2a5e-0ca6-42f0-a7b6-1d3c976b631d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (27.8 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 3,744 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.1 [582 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,744 kB in 0s (28.7 MB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 123642 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (2,653 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123775 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.9 PyMuPDFb-1.24.9\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev\n",
        "!pip install pytesseract\n",
        "!pip install textblob\n",
        "!python -m textblob.download_corpora\n",
        "!pip install pyspellchecker\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image\n",
        "!pip install Pillow\n",
        "!pip install pdfplumber\n",
        "!pip install PyMuPDF\n",
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45CmxpBz8s2c"
      },
      "source": [
        "#**Import modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WYOVtL2mclVD"
      },
      "outputs": [],
      "source": [
        "# Import all required Libraries (For All Over Integration)\n",
        "from pdf2image import convert_from_path\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "import unidecode\n",
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import torch\n",
        "from ipywidgets import widgets as wdg\n",
        "from ipywidgets import widgets\n",
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from IPython.display import display, Audio, clear_output\n",
        "from torch import autocast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg9rGwTQ8xYZ"
      },
      "source": [
        "#**Load summarizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLekAJ2IcPZx",
        "outputId": "0bb1dd64-a2f5-4595-8e21-c5749c1a0488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is not available. Using CPU for computation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model_directory = \"/content/drive/MyDrive/ML02/bart-large-cnn\"\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    # If CUDA is available, set the device to GPU\n",
        "    device = 0  # You can specify which GPU to use by setting device=0 for the first GPU, device=1 for the second GPU, and so on\n",
        "    print(\"CUDA is available. Using GPU for computation.\")\n",
        "else:\n",
        "    # If CUDA is not available, set the device to CPU\n",
        "    device = -1\n",
        "    print(\"CUDA is not available. Using CPU for computation.\")\n",
        "\n",
        "# Importing the necessary library, in this case, the pipeline module from the transformers library\n",
        "from transformers import pipeline\n",
        "\n",
        "# Creating a summarization pipeline using the 'summarization' task and specifying the pre-trained model to be used.\n",
        "# Specify the device parameter to use GPU if available, otherwise fall back to CPU\n",
        "summarizer = pipeline(\"summarization\", model=model_directory, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qaXb_Nc806A"
      },
      "source": [
        "#**File upload**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "NnxG58MBcjmY",
        "outputId": "3a7f2924-1867-40ee-8136-e83e35b5bf66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-08ee8f7d-8db5-4062-b1ce-fd33892e7574\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-08ee8f7d-8db5-4062-b1ce-fd33892e7574\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Registration for Whiteklay Recruitment Drive- 2025 Graduating Batch (1).pdf to Registration for Whiteklay Recruitment Drive- 2025 Graduating Batch (1).pdf\n",
            "User uploaded file \"Registration for Whiteklay Recruitment Drive- 2025 Graduating Batch (1).pdf\" with length 110584 bytes\n",
            "Input File Saved Successfully at :  Registration for Whiteklay Recruitment Drive- 2025 Graduating Batch (1).pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from IPython.display import Audio\n",
        "from IPython.core.display import display\n",
        "\n",
        "#Uploading\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Consider only the first file\n",
        "PATH_TO_YOUR_TEXT = str(list(uploaded.keys())[0])\n",
        "\n",
        "print(\"Input File Saved Successfully at : \", PATH_TO_YOUR_TEXT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lrGb0WtTXBa"
      },
      "source": [
        "#**Text processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ckuFAFdgLU",
        "outputId": "fc53fdbe-2e0a-4803-b9bb-4a57564e4223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEPARTMENT OF TRAINING & PLACEMENT\n",
            "KALINGA INSTITUTE OF INDUSTRIAL TECHNOLOGY (KIIT)\n",
            "DEEMED TO BE UNIVERSITY, BHUBANESWAR (ODISHA)\n",
            "No. KIIT-DU/T&P/24/911\n",
            "Date: 22nd Aug’2024\n",
            "Kind Attenn : 2025 Graduating B.Tech (CSE/CS&SE/CS&CE/IT/ETC/E&CS/EEE)\n",
            "Students\n",
            "This is to inform all the above students that Whiteklay Technologies will be\n",
            "conducting a Recruitment Drive very shortly.\n",
            "Students would have received the Job Notification through Superset & are\n",
            "directed to APPLY on or before 23rd August’2024 by 3.00 PM to participate for\n",
            "the same.\n",
            "Profile Offered\n",
            ":\n",
            "Software Development Engineer\n",
            "Eligibility Criteria\n",
            ":\n",
            "6.00 or above CGPA in B.Tech\n",
            ":\n",
            "No Backlogs\n",
            "Process\n",
            ":\n",
            "To be notified\n",
            "CTC\n",
            ":\n",
            "Rs.9.00 Lakhs Per Annum\n",
            "Job Location\n",
            ":\n",
            "Pune\n",
            "Prof.(Dr.) Prachet Bhuyan\n",
            "Professor & Dean(T&P), KIIT-DU\n",
            "NB:\n",
            "1. The Company will be short listing from the registered students list.\n",
            "2. Find below the Job Description for your reference.\n",
            "Job Description\n",
            "Role and Responsibilities:\n",
            "As a Data Engineer, you will be tasked with creating an ecosystem to have the\n",
            "right data, to ask the right question, at the right time.\n",
            "Develop scalable applications in Scala/Java\n",
            "Build solutions using open source technologies like Apache Spark , Kafka ,\n",
            "Flink etc\n",
            "Maintain up-to-date knowledge on modern big data technologies, explore\n",
            "new platforms and tools.\n",
            "Identify data requirements and influence the design.\n",
            "\n",
            "Influence new computer science platforms to design, analyze and\n",
            "implement complex and new data-driven solutions that impact the\n",
            "company.\n",
            "Find effective solutions to complex data analytics, use-cases using languages,\n",
            "tools, and software to best construct data for predictive modeling and\n",
            "Artificial Intelligence\n",
            "Qualifications Required:\n",
            "2+ years’ experience or relevant project/coursework\n",
            "Experience with two of the following languages: Java, Scala, Python\n",
            "Understanding of distributed systems and programming.\n",
            "Possesses strong communication skills to portray information.\n",
            "Ability to work in an agile environment with high-quality deliverables.\n",
            "Working knowledge of SQL and Relational Databases is a must.\n",
            "Experience with concepts of Hadoop and Spark is desirable but not a must.\n",
            "We at Whiteklay are quite excited to start with the recruitment drive in your\n",
            "college. In lieu of the above I would like to inform you that it is our company\n",
            "policy that if an offer is extended to a student, then they will have to\n",
            "compulsorily accept it and they should not be allowed to sit for any other\n",
            "institute-based recruitment drives. Please ensure that this clause is agreeable\n",
            "to the candidates before we start with the recruitment process. Also, we would\n",
            "be following the below mentioned process for recruitment. Please do not\n",
            "hesitate to write to me or call me in case you have any concerns or queries.\n",
            "College to share the list of interested students to Whiteklay by the 26th of\n",
            "Aug 2024.(it should have their cvs attached as well as mention the branch\n",
            "and CGPA details)\n",
            "Whiteklay will send back the shortlisted list of students who are eligible to\n",
            "sit for the online exam, along with the link for the exam.\n",
            "Online exam will be for 1.5 hrs, during which the students must mandatorily\n",
            "switch on their web cams. They will have to upload a soft copy of their cv as\n",
            "a mandatory field just before they begin the exam on the exam portal itself.\n",
            "So please ask them to keep it handy. This online exam needs to be\n",
            "invigilated by a member of your staff. This is mandatory. Please inform me\n",
            "immediately if you are unable to arrange for the same. We can schedule\n",
            "online exam for 11th Sept 2024 or any other date and time that is\n",
            "convenient to the students. Please confirm the same in your next e mail.\n",
            "After the exam, we would have 2 technical interviews and 1 HR round. The\n",
            "list of shortlisted students will be shared post completion of each round.\n",
            "The interviews would be conducted virtually on Teams meeting, the invites\n",
            "\n",
            "for the same would be provided by me post completion of each round to\n",
            "the shortlisted students.\n",
            "After the Hr round an offer letter would be sent to the selected candidate(s)\n",
            "on their e mail ID(s) through zoho sign within 3 working days.\n",
            "After the candidate has signed the offer letter, it will be shared with the\n",
            "college.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Text Extraction from PDF or Image\n",
        "\n",
        "\n",
        "def extract_text_from_pdf_PyMuPDF(pdf_path):\n",
        "\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    # Open the PDF file\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        # Iterate over each page\n",
        "        for page_num in range(len(pdf)):\n",
        "            # Extract text from the page\n",
        "            page = pdf[page_num]\n",
        "            extracted_text += page.get_text() + \"\\n\"\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "def extract_text(file_path):\n",
        "    if file_path.lower().endswith('.pdf'):\n",
        "        return extract_text_from_pdf_PyMuPDF(file_path)\n",
        "    elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        return extract_text_from_image(file_path)\n",
        "    else:\n",
        "        return \"Unsupported file format.\"\n",
        "\n",
        "# Example usage:\n",
        "file_path = PATH_TO_YOUR_TEXT  # Change this to your file path\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    extracted_text = extract_text(file_path)\n",
        "    print(extracted_text)\n",
        "else:\n",
        "    print(\"File not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yynQ3_7IdoxF",
        "outputId": "c3b4aeb2-02f4-41d7-f142-3f8baf59af97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEPARTMENT OF TRAINING & PLACEMENT KALINGA INSTITUTE OF INDUSTRIAL TECHNOLOGY ( KIIT ) DEEMED TO BE UNIVERSITY, BHUBANESWAR ( ODISHA ) No. KIIT-DU/T & P/24/911 Date: 22nd Aug ’ 2024 Kind Attenn: 2025 Graduating B. Tech ( CSE/CS & SE/CS & CE/IT/ETC/E & CS/EEE ) Students This is to inform all the above students that Whiteklay Technologies will be conducting a Recruitment Drive very shortly. Students would have received the Job Notification through Superset & are directed to APPLY on or before 23rd August ’ 2024 by 3.00 PM to participate for the same. Profile Offered: Software Development Engineer Eligibility Criteria: 6.00 or above CGPA in B. Tech: No Backlogs Process: To be notified CTC: Rs.9.00 Lakhs Per Annum Job Location: Pune Prof. ( Dr. ) Prachet Bhuyan Professor & Dean ( T & P ), KIIT-DU NB: 1. The Company will be shortlisting from the registered students list. 2. Find below the Job Description for your reference. Job Description Role and Responsibilities: As a Data Engineer, you will be tasked with creating an ecosystem to have the right data, to ask the right question, at the right time. Develop scalable applications in Scala/Java Build solutions using open source technologies like Apache Spark, Kafka, Flink etc Maintain up-to-date knowledge on modern big data technologies, explore new platforms and tools. Identify data requirements and influence the design. Influence new computer science platforms to design, analyze and implement complex and new data-driven solutions that impact the company. Find effective solutions to complex data analytics, use-cases using languages, tools, and software to best construct data for predictive modeling and Artificial Intelligence Qualifications Required: 2+ years ’ experience or relevant project/coursework Experience with two of the following languages: Java, Scala, Python Understanding of distributed systems and programming. Possesses strong communication skills to portray information. Ability to work in an agile environment with high-quality deliverables. Working knowledge of SQL and Relational Databases is a must. Experience with concepts of Hadoop and Spark is desirable but not a must. We at Whiteklay are quite excited to start with the recruitment drive in your college. In lieu of the above I would like to inform you that it is our company policy that if an offer is extended to a student, then they will have to compulsorily accept it and they should not be allowed to sit for any other institute-based recruitment drives. Please ensure that this clause is agreeable to the candidates before we start with the recruitment process. Also, we would be following the below mentioned process for recruitment. Please do not hesitate to write tome or call me incase you have any concerns or queries. College to share the list of interested students to Whiteklay by the 26th of Aug 2024. ( it should have their cvs attached as well as mention the branch and CGPA details ) Whiteklay will send back the shortlisted list of students who are eligible to sit for the online exam, along with the link for the exam. Online exam will be for 1.5 hrs, during which the students must mandatorily switch on their web cams. They will have to upload a soft copy of their cv as a mandatory field just before they begin the exam on the exam portal itself. So please ask them to keep it handy. This online exam needs to be invigilated by a member of your staff. This is mandatory. Please inform me immediately if you are unable to arrange for the same. We can schedule online exam for 11th Sept 2024 or any other date and time that is convenient to the students. Please confirm the same in your next email. After the exam, we would have 2 technical interviews and 1 HR round. The list of shortlisted students will be shared post completion of each round. The interviews would be conducted virtually on Teams meeting, the invites for the same would be provided by me post completion of each round to the shortlisted students. After the Hr round an offer letter would be sent to the selected candidate ( s ) on their email ID ( s ) through zoho sign within 3 working days. After the candidate has signed the offer letter, it will be shared with the college.\n"
          ]
        }
      ],
      "source": [
        "## Text Cleaning - 1.1\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def clean_text1(text):\n",
        "    # Remove line breaks and extra whitespaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "\n",
        "    # Join the sentences into a single string with proper spacing\n",
        "    cleaned_text = ' '.join(sentences)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text1(extracted_text)\n",
        "\n",
        "\n",
        "## Text Cleaning - 1.2\n",
        "\n",
        "\n",
        "def clean_text2(text):\n",
        "    # Tokenize the text into words\n",
        "    words = nltk.word_tokenize(cleaned_text)\n",
        "\n",
        "    # Initialize the spell checker\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    # Correct split words\n",
        "    corrected_words = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        word = words[i]\n",
        "        if i < len(words) - 1:\n",
        "            combined_word = word + words[i + 1]\n",
        "            if combined_word in spell:\n",
        "                corrected_words.append(combined_word)\n",
        "                i += 1  # Skip the next word as it's part of the corrected word\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "        i += 1\n",
        "\n",
        "    # Join corrected words into a single string\n",
        "    corrected_text = ' '.join(corrected_words)\n",
        "\n",
        "    # Ensure proper spacing around punctuation\n",
        "    corrected_text = re.sub(r'\\s([?.!,\";:](?:\\s|$))', r'\\1', corrected_text)\n",
        "    corrected_text = re.sub(r'([?.!,\";:])([a-zA-Z])', r'\\1 \\2', corrected_text)\n",
        "\n",
        "    # Tokenize the corrected text into sentences\n",
        "    sentences = nltk.sent_tokenize(corrected_text)\n",
        "\n",
        "    # Join the sentences into a single string with proper spacing\n",
        "    final_text = ' '.join(sentences)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "extracted_text = clean_text2(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKOPbRETcQ6"
      },
      "source": [
        "#**Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW3gxssBdsK2"
      },
      "outputs": [],
      "source": [
        "# Summarization Function\n",
        "\n",
        "def summarized_text(context_data_string):\n",
        "  # Define the maximum number of tokens per chunk\n",
        "  max_tokens_per_chunk = 800  # Reduce the chunk size to have overlapping chunks\n",
        "\n",
        "  # Define the overlap between consecutive chunks\n",
        "  overlap_tokens = 50  # Adjust as needed for better coherence\n",
        "\n",
        "  # Initialize an empty list to store the summaries\n",
        "  summaries = []\n",
        "\n",
        "  # Iterate over the context_data_string in overlapping chunks\n",
        "  start_index = 0\n",
        "  while start_index < len(context_data_string):\n",
        "      # Calculate the end index for the current chunk\n",
        "      end_index = min(start_index + max_tokens_per_chunk, len(context_data_string))\n",
        "\n",
        "      # Extract the current chunk of text\n",
        "      chunk = context_data_string[start_index:end_index]\n",
        "\n",
        "      # Calculate the maximum length for summarization based on the overlap\n",
        "      max_length = min(len(chunk) - overlap_tokens, 80)  # Adjust as needed\n",
        "\n",
        "      # Summarize the current chunk using the summarizer pipeline\n",
        "      summary = summarizer(\n",
        "          chunk,                    # The text to be summarized (chunk of the context).\n",
        "          max_length=max_length,    # The maximum length (in tokens) of the generated summary. Adjusted dynamically based on the length of the chunk.\n",
        "          min_length=50,            # The minimum length (in tokens) of the generated summary. The summarization model will ensure that the summary is at least 50 tokens long.\n",
        "          do_sample=False           # Whether to use sampling or greedy decoding to generate the summary. Setting it to False indicates that greedy decoding will be used, which means selecting the token with the highest probability at each step.\n",
        "      )\n",
        "\n",
        "      # Check if summary is not empty and append it to the list of summaries\n",
        "      if summary:\n",
        "          summaries.append(summary[0]['summary_text'])\n",
        "      else:\n",
        "          print(\"Summarization failed for chunk:\", chunk)\n",
        "\n",
        "      # Move the start index for the next chunk\n",
        "      start_index += max_tokens_per_chunk - overlap_tokens\n",
        "\n",
        "  # Print the concatenated summaries\n",
        "  # print(\"\\nSummary:\")\n",
        "  # Initialize an empty string to store the concatenated summaries\n",
        "  total_summary = \"\"\n",
        "  # Concatenate the summaries together\n",
        "  for index, summary_text in enumerate(summaries):\n",
        "      total_summary += f\"{summary_text}\\n\"\n",
        "  return total_summary\n",
        "\n",
        "\n",
        "summary  = summarized_text(extracted_text)\n",
        "print(\"\\nSUMMARY =>\\n\",summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhdIi0D6TfRq"
      },
      "source": [
        "#**Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltfvkw7KxhEo",
        "outputId": "43c2bd78-a750-49fd-a53a-964842664c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1: Whiteklay Technologies will be conducting a Recruitment Drive very shortly. Students would have received the Job Notification through Superset & are directed to APPLY on or before 23rd August ’ 2024 by 3.00 PM to participate for the same.\n",
            "\n",
            "Chunk 2: Data Engineer will be tasked with creating an ecosystem to have the right data, to ask the right question, at the right time. The Company will be shortlisting from the registered students list. Find below the Job Description for your reference. 2+ years ’ experience or relevant project/coursework. Experience with two of the following languages: Java, Scala, Python.\n",
            "\n",
            "Chunk 3: understanding of distributed systems and programming. Strong communication skills. Ability to work in an agile environment with high-quality deliverables. College to share the list of interested students to Whiteklay by the 26th of Aug 2024.\n",
            "\n",
            "Chunk 4: ( it should have their cvs attached as well as mention the branch and CGPA details ) Whiteklay will send back the shortlisted list of students who are eligible to sit f.  in your college. Online exam will be for 1.5 hrs, during which the students must mandatorily switch on their web cams.\n",
            "\n",
            "Chunk 5: They will have to upload a soft copy of their cv as a mandatory field just before they begin the exam. This online exam needs to be invigilated by a member of your staff. This is mandatory. The list of shortlisted students will be shared post completion of each round.\n",
            "\n",
            "Chunk 6: The interviews would be conducted virtually on Teams meeting, the invites for the same would be provided by me post completion. After the Hr round an offer letter would be sent to the selected candidate ( s ) on their email ID.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## Function - Splitting the Summary into Chunks\n",
        "\n",
        "import nltk\n",
        "# Download the punkt tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to split a summary into chunks of approximately 100 words each\n",
        "def split_summary_into_chunks(summary):\n",
        "    # Tokenize the summary into sentences\n",
        "    sentences = nltk.tokenize.sent_tokenize(summary)\n",
        "    # Initialize an empty list to store the chunks\n",
        "    chunks = []\n",
        "    # Initialize an empty string to store the current chunk\n",
        "    current_chunk = \"\"\n",
        "    # Iterate through each sentence in the summary\n",
        "    for sentence in sentences:\n",
        "        # Check if adding the current sentence to the current chunk will keep its word count below or equal to 80\n",
        "        if len(current_chunk.split()) + len(sentence.split()) <= 60:\n",
        "            # If yes, add the sentence to the current chunk\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            # If adding the sentence exceeds 100 words, add the current chunk to the list of chunks\n",
        "            chunks.append(current_chunk.strip())\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk = sentence\n",
        "    # After iterating through all sentences, if there's any remaining content in the current chunk, add it to the list of chunks\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    # Return the list of chunks\n",
        "    return chunks\n",
        "\n",
        "# Usage:\n",
        "chunks = split_summary_into_chunks(summary)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWBCHg8zh5Sl",
        "outputId": "ef94aef9-8ab6-444a-f336-efd51af2d94b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original document language: \n",
            "English\n",
            "Desired document language: \n",
            "Hindi\n"
          ]
        }
      ],
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "# Language to code mapping\n",
        "language_code_map = {\n",
        "    \"Arabic\": \"ar_AR\",\n",
        "    \"Czech\": \"cs_CZ\",\n",
        "    \"German\": \"de_DE\",\n",
        "    \"English\": \"en_XX\",\n",
        "    \"Spanish\": \"es_XX\",\n",
        "    \"Estonian\": \"et_EE\",\n",
        "    \"Finnish\": \"fi_FI\",\n",
        "    \"French\": \"fr_XX\",\n",
        "    \"Hindi\": \"hi_IN\",\n",
        "    \"Italian\": \"it_IT\",\n",
        "    \"Japanese\": \"ja_XX\",\n",
        "    \"Kazakh\": \"kk_KZ\",\n",
        "    \"Korean\": \"ko_KR\",\n",
        "    \"Lithuanian\": \"lt_LT\",\n",
        "    \"Latvian\": \"lv_LV\",\n",
        "    \"Burmese\": \"my_MM\",\n",
        "    \"Nepali\": \"ne_NP\",\n",
        "    \"Dutch\": \"nl_XX\",\n",
        "    \"Romanian\": \"ro_RO\",\n",
        "    \"Russian\": \"ru_RU\",\n",
        "    \"Sinhala\": \"si_LK\",\n",
        "    \"Turkish\": \"tr_TR\",\n",
        "    \"Vietnamese\": \"vi_VN\",\n",
        "    \"Chinese\": \"zh_CN\",\n",
        "    \"Afrikaans\": \"af_ZA\",\n",
        "    \"Azerbaijani\": \"az_AZ\",\n",
        "    \"Bengali\": \"bn_IN\",\n",
        "    \"Persian\": \"fa_IR\",\n",
        "    \"Hebrew\": \"he_IL\",\n",
        "    \"Croatian\": \"hr_HR\",\n",
        "    \"Indonesian\": \"id_ID\",\n",
        "    \"Georgian\": \"ka_GE\",\n",
        "    \"Khmer\": \"km_KH\",\n",
        "    \"Macedonian\": \"mk_MK\",\n",
        "    \"Malayalam\": \"ml_IN\",\n",
        "    \"Mongolian\": \"mn_MN\",\n",
        "    \"Marathi\": \"mr_IN\",\n",
        "    \"Polish\": \"pl_PL\",\n",
        "    \"Pashto\": \"ps_AF\",\n",
        "    \"Portuguese\": \"pt_XX\",\n",
        "    \"Swedish\": \"sv_SE\",\n",
        "    \"Swahili\": \"sw_KE\",\n",
        "    \"Tamil\": \"ta_IN\",\n",
        "    \"Telugu\": \"te_IN\",\n",
        "    \"Thai\": \"th_TH\",\n",
        "    \"Tagalog\": \"tl_XX\",\n",
        "    \"Ukrainian\": \"uk_UA\",\n",
        "    \"Urdu\": \"ur_PK\",\n",
        "    \"Xhosa\": \"xh_ZA\",\n",
        "    \"Galician\": \"gl_ES\",\n",
        "    \"Slovene\": \"sl_SI\"\n",
        "}\n",
        "print(\"Original document language: \")\n",
        "source_lang_name = input()  # Replace with actual input\n",
        "print(\"Desired document language: \")\n",
        "target_lang_name = input()   # Replace with actual input\n",
        "\n",
        "source_lang = language_code_map.get(source_lang_name)\n",
        "target_lang = language_code_map.get(target_lang_name)\n",
        "\n",
        "# Ensure source and target languages are valid\n",
        "if source_lang is None or target_lang is None:\n",
        "    print(\"Invalid source or target language.\")\n",
        "else:\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "    def translate(source_lang, target_lang, sentence):\n",
        "        # Set the source language\n",
        "        tokenizer.src_lang = source_lang\n",
        "\n",
        "        # Encode the input sentence\n",
        "        encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        generated_tokens = model.generate(\n",
        "            **encoded_sentence,\n",
        "            forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]\n",
        "        )\n",
        "\n",
        "        # Decode and return the translation\n",
        "        return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "s3S1qvJyxlOh",
        "outputId": "1615bc90-fb8b-47ad-a42d-5e7358dceb78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "व्हाइटक्ले प्रौद्योगिकी बहुत जल्द एक भर्ती अभियान चलाएगा। छात्रों को सुपरसेट के माध्यम से नौकरी अधिसूचना प्राप्त होती है और 23 अगस्त 2024 को 3.00 बजे से पहले आवेदन करने के लिए निर्देशित किया जाता है ताकि वे उसी के लिए भागीदारी करें।\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a5721597a170>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f24b333dc282>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(source_lang, target_lang, sentence)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Encode the input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mencoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         generated_tokens = model.generate(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mencoded_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mforced_bos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang_code_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_lang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m                 outputs = self(\n\u001b[0m\u001b[1;32m   2915\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m                     \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1593\u001b[0m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m         )\n\u001b[0;32m-> 1595\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_logits_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0mmasked_lm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "summary = \"\"\n",
        "for i, chunk in enumerate(chunks):\n",
        "    response = translate(source_lang, target_lang, chunk)\n",
        "    summary += response\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L78jc7-68V0m"
      },
      "outputs": [],
      "source": [
        "with open('summary.txt', 'w') as f:\n",
        "    f.write(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJS9gdRDb1vx"
      },
      "source": [
        "#**Deployment**\n",
        "**Insufficient cloud space*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFRhN3Mzg-cV",
        "outputId": "71df820d-122d-4d44-b0db-a866cec22e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.37.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.37.1-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.37.1 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbYOWpwHhHAe",
        "outputId": "61e7bf67-6885-4d03-8e3e-145c4ca96792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from pdf2image import convert_from_path\n",
        "import fitz  # PyMuPDF\n",
        "import unidecode\n",
        "from contextlib import redirect_stdout\n",
        "import os\n",
        "import torch\n",
        "from transformers import pipeline, MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "import PyPDF2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import re\n",
        "import nltk\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load summarization model\n",
        "model_directory = \"/content/drive/MyDrive/ML02/bart-large-cnn\"\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "summarizer = pipeline(\"summarization\", model=model_directory, device=device)\n",
        "\n",
        "# Load translation model\n",
        "# Language to code mapping\n",
        "language_code_map = {\n",
        "    \"Arabic\": \"ar_AR\",\n",
        "    \"Czech\": \"cs_CZ\",\n",
        "    \"German\": \"de_DE\",\n",
        "    \"English\": \"en_XX\",\n",
        "    \"Spanish\": \"es_XX\",\n",
        "    \"Estonian\": \"et_EE\",\n",
        "    \"Finnish\": \"fi_FI\",\n",
        "    \"French\": \"fr_XX\",\n",
        "    \"Hindi\": \"hi_IN\",\n",
        "    \"Italian\": \"it_IT\",\n",
        "    \"Japanese\": \"ja_XX\",\n",
        "    \"Kazakh\": \"kk_KZ\",\n",
        "    \"Korean\": \"ko_KR\",\n",
        "    \"Lithuanian\": \"lt_LT\",\n",
        "    \"Latvian\": \"lv_LV\",\n",
        "    \"Burmese\": \"my_MM\",\n",
        "    \"Nepali\": \"ne_NP\",\n",
        "    \"Dutch\": \"nl_XX\",\n",
        "    \"Romanian\": \"ro_RO\",\n",
        "    \"Russian\": \"ru_RU\",\n",
        "    \"Sinhala\": \"si_LK\",\n",
        "    \"Turkish\": \"tr_TR\",\n",
        "    \"Vietnamese\": \"vi_VN\",\n",
        "    \"Chinese\": \"zh_CN\",\n",
        "    \"Afrikaans\": \"af_ZA\",\n",
        "    \"Azerbaijani\": \"az_AZ\",\n",
        "    \"Bengali\": \"bn_IN\",\n",
        "    \"Persian\": \"fa_IR\",\n",
        "    \"Hebrew\": \"he_IL\",\n",
        "    \"Croatian\": \"hr_HR\",\n",
        "    \"Indonesian\": \"id_ID\",\n",
        "    \"Georgian\": \"ka_GE\",\n",
        "    \"Khmer\": \"km_KH\",\n",
        "    \"Macedonian\": \"mk_MK\",\n",
        "    \"Malayalam\": \"ml_IN\",\n",
        "    \"Mongolian\": \"mn_MN\",\n",
        "    \"Marathi\": \"mr_IN\",\n",
        "    \"Polish\": \"pl_PL\",\n",
        "    \"Pashto\": \"ps_AF\",\n",
        "    \"Portuguese\": \"pt_XX\",\n",
        "    \"Swedish\": \"sv_SE\",\n",
        "    \"Swahili\": \"sw_KE\",\n",
        "    \"Tamil\": \"ta_IN\",\n",
        "    \"Telugu\": \"te_IN\",\n",
        "    \"Thai\": \"th_TH\",\n",
        "    \"Tagalog\": \"tl_XX\",\n",
        "    \"Ukrainian\": \"uk_UA\",\n",
        "    \"Urdu\": \"ur_PK\",\n",
        "    \"Xhosa\": \"xh_ZA\",\n",
        "    \"Galician\": \"gl_ES\",\n",
        "    \"Slovene\": \"sl_SI\"\n",
        "}\n",
        "\n",
        "@st.cache_data\n",
        "def extract_text_from_pdf_PyMuPDF(pdf_path):\n",
        "    extracted_text = \"\"\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        for page_num in range(len(pdf)):\n",
        "            page = pdf[page_num]\n",
        "            extracted_text += page.get_text() + \"\\n\"\n",
        "    return extracted_text\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "@st.cache_data\n",
        "def extract_text(file_path):\n",
        "    if file_path.lower().endswith('.pdf'):\n",
        "        return extract_text_from_pdf_PyMuPDF(file_path)\n",
        "    elif file_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        return extract_text_from_image(file_path)\n",
        "    else:\n",
        "        return \"Unsupported file format.\"\n",
        "\n",
        "def clean_text1(text):\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    sentences = nltk.sent_tokenize(cleaned_text)\n",
        "    cleaned_text = ' '.join(sentences)\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_text2(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    spell = SpellChecker()\n",
        "    corrected_words = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        word = words[i]\n",
        "        if i < len(words) - 1:\n",
        "            combined_word = word + words[i + 1]\n",
        "            if combined_word in spell:\n",
        "                corrected_words.append(combined_word)\n",
        "                i += 1\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "        i += 1\n",
        "    corrected_text = ' '.join(corrected_words)\n",
        "    corrected_text = re.sub(r'\\s([?.!,\";:](?:\\s|$))', r'\\1', corrected_text)\n",
        "    corrected_text = re.sub(r'([?.!,\";:])([a-zA-Z])', r'\\1 \\2', corrected_text)\n",
        "    sentences = nltk.sent_tokenize(corrected_text)\n",
        "    final_text = ' '.join(sentences)\n",
        "    return final_text\n",
        "\n",
        "def summarized_text(context_data_string):\n",
        "    max_tokens_per_chunk = 800\n",
        "    overlap_tokens = 50\n",
        "    summaries = []\n",
        "    start_index = 0\n",
        "    while start_index < len(context_data_string):\n",
        "        end_index = min(start_index + max_tokens_per_chunk, len(context_data_string))\n",
        "        chunk = context_data_string[start_index:end_index]\n",
        "        max_length = min(len(chunk) - overlap_tokens, 80)\n",
        "        summary = summarizer(\n",
        "            chunk,\n",
        "            max_length=max_length,\n",
        "            min_length=50,\n",
        "            do_sample=False\n",
        "        )\n",
        "        if summary:\n",
        "            summaries.append(summary[0]['summary_text'])\n",
        "        start_index += max_tokens_per_chunk - overlap_tokens\n",
        "    total_summary = \"\"\n",
        "    for summary_text in summaries:\n",
        "        total_summary += f\"{summary_text}\\n\"\n",
        "    return total_summary\n",
        "\n",
        "def load_translation_model():\n",
        "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_translation_model()\n",
        "\n",
        "def translate(source_lang, target_lang, sentence):\n",
        "    tokenizer.src_lang = source_lang\n",
        "    encoded_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    generated_tokens = model.generate(\n",
        "        **encoded_sentence,\n",
        "        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]\n",
        "    )\n",
        "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "def split_summary_into_chunks(summary):\n",
        "    sentences = nltk.tokenize.sent_tokenize(summary)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk.split()) + len(sentence.split()) <= 60:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "st.title(\"FileLingo: OCR, Summarization and Translation\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload PDF or Image\", type=['pdf', 'jpg', 'jpeg', 'png'])\n",
        "source_lang_name = st.selectbox(\"Select Source Language\", list(language_code_map.keys()))\n",
        "target_lang_name = st.selectbox(\"Select Target Language\", list(language_code_map.keys()))\n",
        "\n",
        "if st.button(\"Run\"):\n",
        "    if uploaded_file and source_lang_name and target_lang_name:\n",
        "        source_lang = language_code_map.get(source_lang_name)\n",
        "        target_lang = language_code_map.get(target_lang_name)\n",
        "\n",
        "        if source_lang is None or target_lang is None:\n",
        "            st.error(\"Invalid source or target language.\")\n",
        "        else:\n",
        "            file_path = \"/tmp/\" + uploaded_file.name\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "\n",
        "            extracted_text = extract_text(file_path)\n",
        "            cleaned_text = clean_text1(extracted_text)\n",
        "            cleaned_text = clean_text2(cleaned_text)\n",
        "            summary = summarized_text(cleaned_text)\n",
        "            chunks = split_summary_into_chunks(summary)\n",
        "\n",
        "            final_translation = \"\"\n",
        "            for chunk in chunks:\n",
        "                translation = translate(source_lang, target_lang, chunk)\n",
        "                final_translation += translation + \" \"\n",
        "\n",
        "            st.subheader(\"Translated Summary\")\n",
        "            st.write(final_translation)\n",
        "\n",
        "        # with open('/tmp/summary.txt', 'w') as f:\n",
        "        #     f.write(final_translation)\n",
        "        # st.download_button(label=\"Download Summary\", data=open('/tmp/summary.txt', 'r').read(), file_name='summary.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "daQ1hxOahMnk"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6lIfK-zhPsF",
        "outputId": "05b9c16e-0eae-400e-d378-72fd57bd700c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.91.77.142\n",
            "your url is: https://light-ways-deny.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dDkxjG3X8qfR",
        "45CmxpBz8s2c",
        "dg9rGwTQ8xYZ",
        "9qaXb_Nc806A",
        "6lrGb0WtTXBa",
        "sJKOPbRETcQ6",
        "xhdIi0D6TfRq",
        "mJS9gdRDb1vx"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}